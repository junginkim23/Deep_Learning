# -*- coding: utf-8 -*-
"""Classification of MNIST data using CNN model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/113kQSK6eBQixumByzVmaMcoBye0t1qTa
"""

# 관련 패키지 import 
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Device configuration, gpu 사용 가능한 경우 device를 gpu로 설정하고 사용 불가능하면 cpu로 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# Hyper-parameters 
num_classes = 10
num_epochs = 5
batch_size = 100
weight_decay_lambda = 0.01
learning_rate = 0.001

# 파이토치에서 제공하는 MNIST dataset
train_dev_dataset = torchvision.datasets.MNIST(root='./data',train=True, #torchvision은 
                                           transform=transforms.ToTensor(), download=True)
train_dataset, dev_dataset = torch.utils.data.random_split(train_dev_dataset, [50000, 10000])
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, 
                                          transform=transforms.ToTensor())

# 배치 단위로 데이터를 처리해주는 Data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset, 
                                         batch_size=batch_size,
                                         shuffle=False)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                         batch_size=batch_size,
                                         shuffle=False)

class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(64 * 5 * 5, num_classes),
        )
    
    def forward(self, x):
        batch_size = x.shape[0]
        out = self.layer(x)
        out = out.view(batch_size, -1)
        out = self.fc_layer(out)
        return out

model = CNN(num_classes) # 모델을 지정한 device로 올려줌

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_lambda) #optim이라는 패키지 내에 있는 클래스 adam 
#model.parameters #-> 가중치 w들을 의미

def evaluation(data_loader):
    correct = 0
    total = 0
    for images, labels in data_loader:
        images = images
        labels = labels
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    return correct/total

loss_arr = []
max = 0.0
total_step = len(train_loader)

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        model.train()
        # Move tensors to the configured device
        images = images
        labels = labels
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        # Backward and optimize
        optimizer.zero_grad() # iteration 마다 gradient를 0으로 초기화
        loss.backward() # 가중치 w에 대해 loss를 미분
        optimizer.step() # 가중치들을 업데이트

        if (i+1) % 100 == 0:
            loss_arr.append(loss)
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
            with torch.no_grad():
                model.eval()
                acc = evaluation(dev_loader)
                if max < acc :
                    max = acc 
                    print("max dev accuracy: ", max)
                    torch.save(model.state_dict(), 'model.ckpt') #model내에 있는 상태를 디스크 파일에 저장.

type(model.state_dict())

with torch.no_grad(): #파일을 다룰 때, with 블록을 사용하면 close()메소드를 사용하지 않더라도 파일이 종료된다.
#no.grad()를 사용하는 이유: test_data나 dev_data를 사용할때는 forward만 진행하면 되서 forward만 진행하게 해준다.  
    last_acc = evaluation(test_loader)
    print('Last Accuracy of the network on the 10000 test images : {} %'.format(last_acc*100))

    # torch.load('model.ckpt')
    model.load_state_dict(torch.load('model.ckpt')) #실제로 학습한 모델을 호출해준다. 
    best_acc = evaluation(test_loader)
    print('Best Accuracy of the network on the 10000 test images : {} %'.format(best_acc*100))

# Save the model checkpoint
torch.save(model.state_dict(), 'model.ckpt')
plt.plot(loss_arr)
plt.show()