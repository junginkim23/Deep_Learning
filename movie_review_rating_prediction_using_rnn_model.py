# -*- coding: utf-8 -*-
"""Movie review rating prediction using RNN model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yThKvts0px0OPfSGoEAETPE0iuHJnsZ3
"""

import torch
print(torch.__version__) 
# 1.6.0이 안나온다면 아래 코드 실행 필요

#pytorch 및 torchtext 버전 맞추기 
!pip3 install torch==1.6.0+cu101 torchtext==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
# 1회만 실행할 것 
# 실행 후 런타임 재실행 필요

import torch, torchtext
print(torch.__version__)     # 1.6.0+cu101
print(torchtext.__version__) # 0.7.0

from google.colab import drive
drive.mount('/content/drive')

!cp drive/MyDrive/AI\ School/8주차/GoogleNews-vectors-negative300.bin.gz .
!gunzip GoogleNews-vectors-negative300.bin.gz

import os
import collections 
import nltk
nltk.download('punkt')

import numpy as np
import torch.nn as nn
import torch.optim as optim
import tqdm.notebook as tqdm #?

from torch.utils.data import DataLoader, random_split
from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence

from torchtext.experimental.vocab import Vocab
from torchtext.experimental.datasets.text_classification import IMDB #torch내에 구현되어 있는 IMDB 데이터 

from gensim.models.keyedvectors import KeyedVectors

# 뉴럴 네트워크 하이퍼파라미터
num_classes = 2
num_epochs = 5
evaluate_per_steps = 10 #evaluate를 얼마나 자주 할 것이냐
learning_rate = 0.001
dropout_rate = 0.5
batch_size = 50
weight_decay = 5e-4

# Text RNN 하이퍼파라미터
embedding_dim = 300
hidden_size = 100 #성능 올리고 싶다면 이것을 변경하면된다.

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

class TextRnn(nn.Module):
    def __init__(self, num_classes, vocab_size, embedding_dim, pretrained_embedding, hidden_size, dropout_rate):
        super().__init__()
        # embedding layer 선언
        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim) #몇 개의 단어를 몇 개의 차원으로 표현할 것인지.  
        # embedding layer에 pretrained_embedding 넣기 
        self.embedding.weight.copy_(torch.tensor(pretrained_embedding)).detach() #detach()는 이미 학습된 데이터들이니깐 이 부분은 학습하지 않고 고정시켜둘때 사용하는 것 
        #'_' 사용의 의미: copy()를 통해 나온 값을 바로 weight에 넣어주겠다. self.embedding.weight = copy()이렇게 않하고 
        # rnn layer 선언 
        self.rnn = nn.RNN(input_size=embedding_dim,hidden_size=hidden_size)
        # dropout layer 선언 
        self.dropout = nn.Dropout(dropout_rate)
        # fc layer 선언 
        self.fc = nn.Linear(in_features=hidden_size,out_features=num_classes)
    def forward(self, text, length):
        out = self.embedding(text) # 3(apple) -> [6,2,3,4,5,6,2,...] -> 300차원
        out = pack_padded_sequence(out, lengths=length)
        all_outs, final_out = self.rnn(out) #final_out의 size : [1,batch,hidden_size]
        out = final_out[0] 
        print(out.size())
        out = self.dropout(out)
        out = self.fc(out)
        return out

# batch:
# [
#   (tensor(0 or 1), tensor([3, 4, 13, 38, 20]),    5 단어 #tensor([3,4,13,38,20])에서 []값은 벡터의 형태 
#   (tensor(0 or 1), tensor([21, 21, 3, 76]),       4 단어
#   (tensor(0 or 1), tensor([53, 48, 1]),           3 단어 
#   (tensor(0 or 1), tensor([3, 4, 4, 1, 3, 20]),   6 단어
# ]  zip을 해줄때 단어를 가리키는 index의 크기는 다 달라서 최대 크기로 맞춰준다. 이 때 빈 공간에는 쓰레기 값을 넣어주는데 주로 그 값은 0번에 indexing되어 있고 그 값은 <pad>라고 한다. 
#zip을 진행할때는 단어들의 크기를 동일하게 맞추는 작업을 하진 않는가보다. 나중에 후에 맞춰주는 작업이 따로 있는듯. 
def collate(batch): #batch단위로 작업을 한다. 
    labels, text = zip(*batch) 
    #if a=[[1,2],[3,4],[5,6]]
    #print(*a) output:[1,2] [3,4] [5,6] -> '*'를 붙이면 unpacking된다. 
    #zip(*a) #output: [1,3,5] [2,4,6] -> 'zip'은 파이썬 내장함수인데, 같은 index에 있는 값끼리 모은다. 

    lengths = []
    for t in text: #text내에 어디까지가 진짜 단어이고 어디가 쓰레기값인지를 알기 위해서 lengths라는 리스트에 각각의 데이터마다 단어들의 크기를 넣어준다. 
        lengths.append(len(t))

    text = pad_sequence(text) # 가장 긴 text를 기준으로 직사각형으로 만들어 줌, pad_sequence라는 torch에 있는 함수를 사용하여 pad를 크기에 맞게 채워준다. 
    lengths = torch.tensor(lengths) # List to tensor
    labels = torch.stack(labels) # stack : 1개의 차원을 추가해주면서 합치는 것 / 0 dim -> 1 dim(tensor) 지금 label에 있는 값은 벡터가 아니라 int형이다. 그래서 나중에 네트워크에 넣어줄때는 벡터의 형태로 넣어줘야해서 벡터로 바꿔준다. 
    print(labels.size(),text.size(),lengths.size())
    return labels, text, lengths

def evaluate(model, data_loader):
    with torch.no_grad():
        model.eval()
        num_corrects = 0
        num_total = 0
        for label, text, length in data_loader:
            label = label.to(device)
            text = text.to(device)
            length = length.to(device)

            output = model(text, length)
            predictions = output.argmax(dim=-1)
            num_corrects += (predictions == label).sum().item()
            num_total += label.size(0)
            
        return num_corrects / num_total

def get_pre_trained_emb(vocab, embedding_dim, pre_trained_path):
    print('start to load google word2vec model')
    google_word2vec = KeyedVectors.load_word2vec_format(pre_trained_path, binary=True, limit=60000) #binary는 빈 파일이니깐 True를 넣어준다. 저번시간에 한 거 참조! 
    #google_word2vec -> 이 matrix는 우리가 만들어주려 하는 weight_matrix를 위해 필요한 matrix 
    print('Done!')

    matrix_len = len(vocab)
    weights_matrix = np.zeros((matrix_len, embedding_dim))
    words_found = 0 #google_word2vec에서 몇 개를 찾았는지 count해주는 변수 

    for i, word in enumerate(vocab.itos): # itos : Index TO String
        try: 
            weights_matrix[i] = google_word2vec[word] #해당하는 단어가 google_word2vec에 있다면 그 단어에 해당하는 vector의 값을 weight_matrix에 넣어준다.
            words_found += 1 
        except KeyError:
            weights_matrix[i] = np.random.uniform(-0.2, 0.2, size=google_word2vec.vector_size) #해당하는 단어가 없다면, -0.2~0.2 사이의 균일하게 랜덤한 값을 넣어준다. 
            #이렇게 해당되는 단어가 없을 때 random한 값으로 넣어주는 이유는? 
    print(f'total vocab length : {matrix_len}, matched word count : {words_found}')
    return weights_matrix

# 데이터 준비 
train_dataset, test_dataset = IMDB(tokenizer=None, data_select=("train", "test"))

vocab = train_dataset.get_vocab() #train_dataset에 내장되어 있는 get_vocab()이라는 함수를 사용하여 vocab을 가지고 온다. 
print(type(vocab))
num_train = int(len(train_dataset)*0.9)
num_dev = len(train_dataset) - num_train

train_dataset, dev_dataset = random_split(train_dataset, (num_train, num_dev))
print(f'dataset size train / dev / test : {len(train_dataset)} / {len(dev_dataset)} / {len(test_dataset)}')

train_loader = DataLoader(train_dataset, batch_size=batch_size    , num_workers=8, collate_fn=collate) #dataset마다 get.item() 함수를 사용해서 worker들이 데이터를 가지고 온다. 
#worker들이 dataset에 정의된 방식대로 데이터를 불러와 쌓는다. 
#DataLoader가 하는 일은 dataset에서 불러온 데이터가 batch_size만큼 쌓이게 되면 쌓인 그 데이터들을 기다리고 있는 네트워크로 옮겨주는 작업을 한다. 
#DataLoader가 네트워크로 데이터를 옮겨 줄때의 조건은 데이터의 크기가 일정해야 한다. 
#만약 데이터의 크기가 다르다면, 에러가 난다. 이 때 collate_fn이 하는 역할은 크기가 다른 것을 일정한 크기로 설정해주는 작업을 해준다. 
dev_loader   = DataLoader(dev_dataset,   batch_size=batch_size * 4, num_workers=8, collate_fn=collate)
test_loader  = DataLoader(test_dataset,  batch_size=batch_size * 4, num_workers=8, collate_fn=collate)

# pre trained word embedding : Google word2vec 
vectors = get_pre_trained_emb(vocab=vocab, 
                              embedding_dim=embedding_dim, 
                              pre_trained_path='GoogleNews-vectors-negative300.bin')

# 모델 선언 
model = TextRnn(num_classes=num_classes,
                embedding_dim=embedding_dim,
                vocab_size=len(vocab),
                pretrained_embedding=vectors,
                hidden_size=hidden_size,
                dropout_rate=dropout_rate).to(device)

# 로스, 옵티마이저 정의
optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)
criterion = nn.CrossEntropyLoss()

# 학습 진행
steps = 0
max_dev_accuracy = 0.0

for epoch in tqdm.trange(num_epochs):
    progress = tqdm.tqdm(train_loader)
    for label, text, length in progress:
        model.train()
        steps += 1
        label = label.to(device)
        text = text.to(device)
        length = length.to(device)

        output = model(text, length)
        loss = criterion(output, label)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        progress.set_description(f'train loss: {loss.item():.4f}')

        if steps % evaluate_per_steps == 0:
            print('***** evaluating on the dev set *****')
            dev_accuracy = evaluate(model, dev_loader)
            print(f'dev accuracy: {dev_accuracy:.4f}')
            if dev_accuracy > max_dev_accuracy:
                max_dev_accuracy = dev_accuracy
                print('achieve dev-best accuracy. saving.')
                torch.save(model.state_dict(), 'best_weight.pt')

# 평가
print('***** evaluating dev-best on the test set *****')
model.load_state_dict(torch.load('best_weight.pt'))
test_accuracy = evaluate(model, test_loader)
print(f'test accuracy: {test_accuracy}')

