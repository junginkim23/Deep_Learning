# -*- coding: utf-8 -*-
"""Attention mechanism for NMT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yPxfhbm2i47z6MLNi-w7Fg2B2qjXy1PH
"""

# Drive 마운트 
# 학습에 사용할 데이터를 본인의 Drive에 업로드하고 해당 위치를 적어주기 
!unzip drive/MyDrive/AI_School/12기_기초반/강의\ 코드/sources/seq2seq_NMTdata.zip -d .
!rm -r __MACOSX

"""아래 코드는 pytorch seq2seq 공식 튜토리얼 문서를 참조했습니다. 

https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
"""

from io import open
import unicodedata
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
"""# 데이터 불러오기"""

SOS_token = 0 # Start of Sentence 
EOS_token = 1 # End of Sentence

class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS"}
        self.n_words = 2  # SOS & EOS 카운트 

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# 소문자, trim, unicode -> Ascii and 문자가 아닌 글자 제거
def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

def readLangs(lang1, lang2, reverse=False):
    print("Reading lines...")

    # 파일 읽고 줄 단위로 나누기 
    lines = open('seq2seq_NMTdata/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
        read().strip().split('\n')

    # 각 줄을 pairs 단위로 나누고 normalize 함수 진행 
    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]

    # Lang 객체 생성, 필요시 reverse 진행
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

MAX_LENGTH = 10

eng_prefixes = (
    "i am ", "i m ",
    "he is", "he s ",
    "she is", "she s ",
    "you are", "you re ",
    "we are", "we re ",
    "they are", "they re "
)


def filterPair(p):
    return len(p[0].split(' ')) < MAX_LENGTH and \
        len(p[1].split(' ')) < MAX_LENGTH and \
        p[1].startswith(eng_prefixes)


def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

# 1. 파일을 읽어서 줄로 나누고 pair로 나누기 
# 2. normalize 시키고 pair를 filter에 통과시키기(Max legnth와 prefixes에 맞는 애들로)
# 3. Pairs 안에 있는 문장으로 단어 리스트 생성 
def prepareData(lang1, lang2, reverse=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)
    print("Read %s sentence pairs" % len(pairs))
    pairs = filterPairs(pairs)
    print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


input_lang, output_lang, pairs = prepareData('eng', 'fra', True)
print(random.choice(pairs))

"""# Encoder

입력으로 들어오는 언어의 내용을 잘 간추려 하나의 vector로 만듦
"""

class EncoderLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderLSTM, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)

    def forward(self, input, hidden, cell):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, (hidden, cell) = self.lstm(output, (hidden, cell))
        return output, hidden, cell

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

"""# Additive attention Decoder

Encoder에서 만들어 낸 vector를 이용해 additive attention mechanism 기법을 이용해 출력 단어를 뱉어줌
"""

class AttentionDecoderLSTM(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(AttentionDecoderLSTM, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)
        
        self.atten_h = nn.Linear(hidden_size, hidden_size)
        self.atten_s = nn.Linear(hidden_size, hidden_size)
        self.atten_v = nn.Linear(hidden_size, 1)
        
        self.out = nn.Linear(hidden_size * 2, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden, cell, encoder_outputs):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, (hidden, cell) = self.lstm(output, (hidden, cell)) # output : [1, 1, hidden_size]
        
        atten_h = self.atten_h(encoder_outputs) # [10, hidden_size]
        atten_s = self.atten_s(output[0]) # [1, hidden_size]
        attention_scrs = self.atten_v(torch.tanh(torch.add(atten_h, atten_s))) # [max_length, 1]
        attention_dist = F.softmax(attention_scrs, dim=0) # [max_length, 1]
        attention_out = torch.mul(encoder_outputs, attention_dist) # [max_length, hidden_size]
        attention_out = torch.sum(attention_out, dim=0) # [hidden_size]
        
        output = torch.cat((output[0], attention_out.unsqueeze(0)),dim=1) # [1, hidden_size*2]
        output = self.softmax(self.out(output)) # [1, output_size]

        return output, hidden, cell, attention_dist

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

"""# 한 pair의 Train 과정

전체 학습 데이터가 아니라 language1과 language2 데이터 한 쌍의 학습 과정

lang1을 encoding 하고 lang2로 decoding해서 loss를 구하고 각각의 optimizer로 update
"""

teacher_forcing_ratio = 0.5

def train(input_tensor, target_tensor, encoder, decoder, \
          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()
    encoder_cell = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)
    
    encoder_outputs = torch.zeros(max_length, encoder.hidden_size).to(device)
    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], 
                                                               encoder_hidden,
                                                               encoder_cell)
        encoder_outputs[ei] = encoder_output[0, 0]
    
    decoder_input = torch.tensor([[SOS_token]], device=device)
    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    loss = 0
    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False
    if use_teacher_forcing:
        # Teacher forcing: 네트워크가 뱉어낸 출력물과는 상관없이 실제 정답을 decoder의 다음 입력으로 사용
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, 
                                                                      decoder_hidden,
                                                                      decoder_cell,
                                                                      encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: 네트워크가 뱉어낸 출력물 자체를 decoder의 다음 입력으로 사용
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, 
                                                                      decoder_hidden,
                                                                      decoder_cell,
                                                                      encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()
    encoder_optimizer.step()
    decoder_optimizer.step()
    return loss.item() / target_length

"""# Helper 함수

진행도와 남은 학습 상황을 보여주는 도우미 함수
"""

import time
import math

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

"""# PLT show 함수"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np
# %matplotlib inline

def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)

"""# 전체 Train 과정

전체 학습 과정을 담당하는 코드 

다음의 과정이 담겨있다

1. 타이머 시작 세팅 
2. Encoder와 Decoder optimizer 선언 
3. Loss 선언 
4. 전체 데이터를 학습에 사용할 수 있는 형태로 불러오기 
5. 각 데이터 pair를 train 시키기 
6. 전체 loss update 
7. 시각화
"""

def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] for word in sentence.split(' ')]

def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)

def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0   # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]
    criterion = nn.NLLLoss()

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]

        loss = train(input_tensor, target_tensor, encoder, decoder, 
                     encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

    showPlot(plot_losses)

"""# 실제 학습 시작 지점"""

hidden_size = 256
n_iter = 150000
print_every = 2500

encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)
decoder = AttentionDecoderLSTM(hidden_size, output_lang.n_words).to(device)

trainIters(encoder, decoder, n_iter, print_every=print_every)

"""# Evaluate

Decoding과정에서 만들어진 t 시점의 출력물이 t+1 시점의 입력으로 들어가도록 설계
"""

def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()
        encoder_cell = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size).to(device)
        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei],
                                                                   encoder_hidden,
                                                                   encoder_cell)
            encoder_outputs[ei] = encoder_output[0, 0]
        
        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell

        decoded_words = []
        attention_dists = torch.zeros(max_length, max_length)
        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_cell, attention_dist = decoder(decoder_input, 
                                                                                   decoder_hidden,
                                                                                   decoder_cell,
                                                                                   encoder_outputs)

            attention_dists[di] = attention_dist.squeeze().detach()
            topv, topi = decoder_output.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, attention_dists[:di+1]

def evaluateRandomly(encoder, decoder, n=5):
    for i in range(n):
        pair = random.choice(pairs)
        print('>', pair[0])
        print('=', pair[1])
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

evaluateRandomly(encoder, decoder)

sentence = 'il est sur le bureau .'
output_words, attentions = evaluate(encoder, decoder, sentence)
output_sentence = ' '.join(output_words)
print('>', sentence)
print('<', output_sentence)

plt.matshow(attentions.numpy())
plt.colorbar()
plt.show()

"""# Attention 시각화"""

def showAttention(input_sentence, output_words, attentions):
    # colorbar로 그림 설정
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions.numpy(), cmap='bone')
    fig.colorbar(cax)

    # 축 설정
    ax.set_xticklabels([''] + input_sentence.split(' ') +
                       ['<EOS>'], rotation=90)
    ax.set_yticklabels([''] + output_words)

    # 매 틱마다 라벨 보여주기
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()


def evaluateAndShowAttention(input_sentence):
    output_words, attentions = evaluate(encoder, decoder, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))
    showAttention(input_sentence, output_words, attentions)

evaluateAndShowAttention("elle a cinq ans de moins que moi .")

evaluateAndShowAttention("elle est trop petit .")

evaluateAndShowAttention("je ne crains pas de mourir .")

evaluateAndShowAttention("c est un jeune directeur plein de talent .")

